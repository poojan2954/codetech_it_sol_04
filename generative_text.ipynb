{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7ed51dd-2bdc-435c-8fe1-046e850a3cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\poojan\\anaconda3\\lib\\site-packages (4.48.2)\n",
      "Requirement already satisfied: torch in c:\\users\\poojan\\anaconda3\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\poojan\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\poojan\\anaconda3\\lib\\site-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\poojan\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\poojan\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\poojan\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\poojan\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\poojan\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\poojan\\anaconda3\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\poojan\\anaconda3\\lib\\site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\poojan\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\poojan\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\poojan\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\poojan\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\poojan\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\poojan\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\poojan\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\poojan\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\poojan\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\poojan\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\poojan\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\poojan\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\poojan\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\poojan\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.12.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71baa70f-7600-471b-812b-932d30453556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\poojan\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "409f973574984ff48d4e8e3bb710a4da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\poojan\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\poojan\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99b61757cabb4068a05ee5185b351b28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8acbf547fb34081a15097c5619d00e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "507eafe0011a48c29f4fbf6b5b8b71b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fe8ed05fd3943d589f38608588985aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6dda501f59d45ca82516d99e61f473d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00eaffaa272742eb98b95f5dfe078ab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Choose a pretrained GPT model (GPT-2 in this example)\n",
    "model_name = \"gpt2\"  # You can try \"gpt2-medium\", \"gpt2-large\", etc. for larger models\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model.eval()  # Set the model to evaluation mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d30ed32-b331-46d5-9385-82d67404fad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_length=200, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"\n",
    "    Generate a coherent paragraph based on the given prompt.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The input prompt (e.g., topic or starting sentence).\n",
    "        max_length (int): Maximum length of the generated text.\n",
    "        temperature (float): Sampling temperature (higher values = more random outputs).\n",
    "        top_p (float): Nucleus sampling probability threshold.\n",
    "\n",
    "    Returns:\n",
    "        str: Generated text.\n",
    "    \"\"\"\n",
    "    # Encode the prompt text to tensor\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Generate text using the model\n",
    "    output_sequences = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        do_sample=True,       # Set to True for sampling\n",
    "        num_return_sequences=1  # Number of generated sequences\n",
    "    )\n",
    "    \n",
    "    # Decode and return the generated text\n",
    "    generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a104268-4460-4299-90ed-65c859b08bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "\n",
      "The impact of climate change on polar bears in the Arctic region, and on their natural habitat, is still unclear.\n",
      "\n",
      "\"It's a very difficult question,\" said David C. McEwen, a senior research scientist at the University of Maryland's Center for Polar Studies. \"But it's a very good question.\"\n",
      "\n",
      "The researchers used data from the National Oceanic and Atmospheric Administration's Polar Research Program, which collects data from NOAA's Polar Research Program to monitor the effects of climate change on polar bears in the Arctic.\n",
      "\n",
      "The Polar Research Program tracks how polar bears are affected by their natural habitat, including their ability to adapt to their environment, their ability to adapt to changes in climate and their ability to migrate to other regions.\n",
      "\n",
      "The Polar Research Program is funded by the U.S. Department of Agriculture's National Oceanic and Atmospheric Administration and the National Science Foundation.\n",
      "\n",
      "The study was published online June 27 in the journal Science Advances.\n",
      "\n",
      "The study was led by McEwen, who is also the principal investigator for the study.\n",
      "\n",
      "\"This is the first study that we've looked at the impact of climate change on polar bears,\" he said. \"It's the first study that we\n"
     ]
    }
   ],
   "source": [
    "# Example prompt: you can change this to any specific topic\n",
    "user_prompt = \"The impact of climate change on polar bears in the Arctic region\"\n",
    "\n",
    "generated_paragraph = generate_text(user_prompt, max_length=250)\n",
    "print(\"Generated Text:\\n\")\n",
    "print(generated_paragraph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "034cee76-b9d7-4d5b-b9d5-aec9da507bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a topic or starting sentence:  \"The dawn of renewable energy is reshaping urban landscapes in unprecedented ways.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text:\n",
      "\n",
      "\"The dawn of renewable energy is reshaping urban landscapes in unprecedented ways.\"\n",
      "\n",
      "In the early 2000s, the City of London, a major donor to the Green Alliance, announced that it would build a new renewable energy network at the heart of its London skyline.\n",
      "\n",
      "But the Green Alliance's chief executive, James Hill, insisted that the network would be built on a \"new, renewable\" basis.\n",
      "\n",
      "He said: \"The Green Alliance has a very clear vision for London's future, and we are committed to building a network of infrastructure that will provide a new, renewable future for our city.\n",
      "\n",
      "\"In fact, the Green Alliance is committed to building a network of infrastructure that will provide a new, renewable future for our city, and we are committed to building a network of infrastructure that will provide a new, renewable future for our city.\n",
      "\n",
      "\"As we continue to grow and advance in technology, our commitment to the green future is becoming increasingly evident.\"\n",
      "\n",
      "The Green Alliance said its network would be built on the \"new, renewable\" basis.\n",
      "\n",
      "The organisation said it would be built on a \"new, renewable\" basis.\n",
      "\n",
      "Its chairman, Dr Norman Thomas, said: \"We\n"
     ]
    }
   ],
   "source": [
    "# Interactive prompt for the user\n",
    "user_prompt = input(\"Enter a topic or starting sentence: \")\n",
    "generated_paragraph = generate_text(user_prompt, max_length=250)\n",
    "print(\"\\nGenerated Text:\\n\")\n",
    "print(generated_paragraph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3330c7f-f7ae-4b20-a52d-6249e668806d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
